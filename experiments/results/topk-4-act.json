{
  "arc_challenge": {
    "alias": "arc_challenge",
    "acc,none": 0.17491467576791808,
    "acc_stderr,none": 0.011101562501828297,
    "acc_norm,none": 0.22866894197952217,
    "acc_norm_stderr,none": 0.012272853582540892
  },
  "hellaswag": {
    "alias": "hellaswag",
    "acc,none": 0.2818163712407887,
    "acc_stderr,none": 0.004489648865081343,
    "acc_norm,none": 0.3229436367257518,
    "acc_norm_stderr,none": 0.004666457279979286
  },
  "mmlu": {
    "acc,none": 0.253525138869107,
    "acc_stderr,none": 0.00366855602364698,
    "alias": "mmlu"
  },
  "mmlu_humanities": {
    "acc,none": 0.2541976620616366,
    "acc_stderr,none": 0.006346203926607209,
    "alias": " - humanities"
  },
  "mmlu_formal_logic": {
    "alias": "  - formal_logic",
    "acc,none": 0.3253968253968254,
    "acc_stderr,none": 0.04190596438871138
  },
  "mmlu_high_school_european_history": {
    "alias": "  - high_school_european_history",
    "acc,none": 0.24242424242424243,
    "acc_stderr,none": 0.03346409881055956
  },
  "mmlu_high_school_us_history": {
    "alias": "  - high_school_us_history",
    "acc,none": 0.25980392156862747,
    "acc_stderr,none": 0.030778554678693247
  },
  "mmlu_high_school_world_history": {
    "alias": "  - high_school_world_history",
    "acc,none": 0.27848101265822783,
    "acc_stderr,none": 0.029178682304842548
  },
  "mmlu_international_law": {
    "alias": "  - international_law",
    "acc,none": 0.24793388429752067,
    "acc_stderr,none": 0.03941897526516304
  },
  "mmlu_jurisprudence": {
    "alias": "  - jurisprudence",
    "acc,none": 0.24074074074074073,
    "acc_stderr,none": 0.041331194402438355
  },
  "mmlu_logical_fallacies": {
    "alias": "  - logical_fallacies",
    "acc,none": 0.26380368098159507,
    "acc_stderr,none": 0.03462419931615622
  },
  "mmlu_moral_disputes": {
    "alias": "  - moral_disputes",
    "acc,none": 0.25722543352601157,
    "acc_stderr,none": 0.02353292543104432
  },
  "mmlu_moral_scenarios": {
    "alias": "  - moral_scenarios",
    "acc,none": 0.24134078212290502,
    "acc_stderr,none": 0.014310999547961547
  },
  "mmlu_philosophy": {
    "alias": "  - philosophy",
    "acc,none": 0.22186495176848875,
    "acc_stderr,none": 0.023598858292862998
  },
  "mmlu_prehistory": {
    "alias": "  - prehistory",
    "acc,none": 0.21604938271604937,
    "acc_stderr,none": 0.02289916291844576
  },
  "mmlu_professional_law": {
    "alias": "  - professional_law",
    "acc,none": 0.258148631029987,
    "acc_stderr,none": 0.011176923719313468
  },
  "mmlu_world_religions": {
    "alias": "  - world_religions",
    "acc,none": 0.3333333333333333,
    "acc_stderr,none": 0.036155076303109365
  },
  "mmlu_other": {
    "acc,none": 0.2587705181847441,
    "acc_stderr,none": 0.007831155071912366,
    "alias": " - other"
  },
  "mmlu_business_ethics": {
    "alias": "  - business_ethics",
    "acc,none": 0.3,
    "acc_stderr,none": 0.04605661864718382
  },
  "mmlu_clinical_knowledge": {
    "alias": "  - clinical_knowledge",
    "acc,none": 0.2490566037735849,
    "acc_stderr,none": 0.026616482980501673
  },
  "mmlu_college_medicine": {
    "alias": "  - college_medicine",
    "acc,none": 0.20809248554913296,
    "acc_stderr,none": 0.030952890217749857
  },
  "mmlu_global_facts": {
    "alias": "  - global_facts",
    "acc,none": 0.15,
    "acc_stderr,none": 0.035887028128263665
  },
  "mmlu_human_aging": {
    "alias": "  - human_aging",
    "acc,none": 0.30493273542600896,
    "acc_stderr,none": 0.03089861088247754
  },
  "mmlu_management": {
    "alias": "  - management",
    "acc,none": 0.2815533980582524,
    "acc_stderr,none": 0.044532548363264673
  },
  "mmlu_marketing": {
    "alias": "  - marketing",
    "acc,none": 0.3547008547008547,
    "acc_stderr,none": 0.03134250486245399
  },
  "mmlu_medical_genetics": {
    "alias": "  - medical_genetics",
    "acc,none": 0.31,
    "acc_stderr,none": 0.04648231987117317
  },
  "mmlu_miscellaneous": {
    "alias": "  - miscellaneous",
    "acc,none": 0.25925925925925924,
    "acc_stderr,none": 0.01567100600933955
  },
  "mmlu_nutrition": {
    "alias": "  - nutrition",
    "acc,none": 0.2679738562091503,
    "acc_stderr,none": 0.025360603796242578
  },
  "mmlu_professional_accounting": {
    "alias": "  - professional_accounting",
    "acc,none": 0.24113475177304963,
    "acc_stderr,none": 0.025518731049537818
  },
  "mmlu_professional_medicine": {
    "alias": "  - professional_medicine",
    "acc,none": 0.18382352941176472,
    "acc_stderr,none": 0.02352924218519312
  },
  "mmlu_virology": {
    "alias": "  - virology",
    "acc,none": 0.25903614457831325,
    "acc_stderr,none": 0.034106466140718515
  },
  "mmlu_social_sciences": {
    "acc,none": 0.25706857328566785,
    "acc_stderr,none": 0.007885451908147944,
    "alias": " - social sciences"
  },
  "mmlu_econometrics": {
    "alias": "  - econometrics",
    "acc,none": 0.21929824561403508,
    "acc_stderr,none": 0.038924311065187546
  },
  "mmlu_high_school_geography": {
    "alias": "  - high_school_geography",
    "acc,none": 0.2676767676767677,
    "acc_stderr,none": 0.031544498882702825
  },
  "mmlu_high_school_government_and_politics": {
    "alias": "  - high_school_government_and_politics",
    "acc,none": 0.23316062176165803,
    "acc_stderr,none": 0.03051611137147603
  },
  "mmlu_high_school_macroeconomics": {
    "alias": "  - high_school_macroeconomics",
    "acc,none": 0.258974358974359,
    "acc_stderr,none": 0.02221110681006163
  },
  "mmlu_high_school_microeconomics": {
    "alias": "  - high_school_microeconomics",
    "acc,none": 0.25210084033613445,
    "acc_stderr,none": 0.02820554503327774
  },
  "mmlu_high_school_psychology": {
    "alias": "  - high_school_psychology",
    "acc,none": 0.25504587155963304,
    "acc_stderr,none": 0.018688500856535898
  },
  "mmlu_human_sexuality": {
    "alias": "  - human_sexuality",
    "acc,none": 0.25190839694656486,
    "acc_stderr,none": 0.03807387116306089
  },
  "mmlu_professional_psychology": {
    "alias": "  - professional_psychology",
    "acc,none": 0.26143790849673204,
    "acc_stderr,none": 0.017776947157528176
  },
  "mmlu_public_relations": {
    "alias": "  - public_relations",
    "acc,none": 0.3,
    "acc_stderr,none": 0.04389311454644288
  },
  "mmlu_security_studies": {
    "alias": "  - security_studies",
    "acc,none": 0.22040816326530613,
    "acc_stderr,none": 0.026537045312145312
  },
  "mmlu_sociology": {
    "alias": "  - sociology",
    "acc,none": 0.2885572139303483,
    "acc_stderr,none": 0.03203841040213321
  },
  "mmlu_us_foreign_policy": {
    "alias": "  - us_foreign_policy",
    "acc,none": 0.3,
    "acc_stderr,none": 0.04605661864718382
  },
  "mmlu_stem": {
    "acc,none": 0.24389470345702505,
    "acc_stderr,none": 0.007647108891265585,
    "alias": " - stem"
  },
  "mmlu_abstract_algebra": {
    "alias": "  - abstract_algebra",
    "acc,none": 0.23,
    "acc_stderr,none": 0.04229525846816507
  },
  "mmlu_anatomy": {
    "alias": "  - anatomy",
    "acc,none": 0.2222222222222222,
    "acc_stderr,none": 0.03591444084196968
  },
  "mmlu_astronomy": {
    "alias": "  - astronomy",
    "acc,none": 0.2565789473684211,
    "acc_stderr,none": 0.035541803680256875
  },
  "mmlu_college_biology": {
    "alias": "  - college_biology",
    "acc,none": 0.2916666666666667,
    "acc_stderr,none": 0.03800968060554863
  },
  "mmlu_college_chemistry": {
    "alias": "  - college_chemistry",
    "acc,none": 0.23,
    "acc_stderr,none": 0.04229525846816507
  },
  "mmlu_college_computer_science": {
    "alias": "  - college_computer_science",
    "acc,none": 0.35,
    "acc_stderr,none": 0.04793724854411023
  },
  "mmlu_college_mathematics": {
    "alias": "  - college_mathematics",
    "acc,none": 0.25,
    "acc_stderr,none": 0.04351941398892446
  },
  "mmlu_college_physics": {
    "alias": "  - college_physics",
    "acc,none": 0.21568627450980393,
    "acc_stderr,none": 0.04092563958237658
  },
  "mmlu_computer_security": {
    "alias": "  - computer_security",
    "acc,none": 0.3,
    "acc_stderr,none": 0.04605661864718382
  },
  "mmlu_conceptual_physics": {
    "alias": "  - conceptual_physics",
    "acc,none": 0.2680851063829787,
    "acc_stderr,none": 0.02895734278834235
  },
  "mmlu_electrical_engineering": {
    "alias": "  - electrical_engineering",
    "acc,none": 0.22758620689655173,
    "acc_stderr,none": 0.03493950380131187
  },
  "mmlu_elementary_mathematics": {
    "alias": "  - elementary_mathematics",
    "acc,none": 0.21693121693121692,
    "acc_stderr,none": 0.021227082449445076
  },
  "mmlu_high_school_biology": {
    "alias": "  - high_school_biology",
    "acc,none": 0.23225806451612904,
    "acc_stderr,none": 0.024022256130308235
  },
  "mmlu_high_school_chemistry": {
    "alias": "  - high_school_chemistry",
    "acc,none": 0.20689655172413793,
    "acc_stderr,none": 0.028501378167893957
  },
  "mmlu_high_school_computer_science": {
    "alias": "  - high_school_computer_science",
    "acc,none": 0.26,
    "acc_stderr,none": 0.0440844002276808
  },
  "mmlu_high_school_mathematics": {
    "alias": "  - high_school_mathematics",
    "acc,none": 0.22962962962962963,
    "acc_stderr,none": 0.025644108639267627
  },
  "mmlu_high_school_physics": {
    "alias": "  - high_school_physics",
    "acc,none": 0.2251655629139073,
    "acc_stderr,none": 0.03410435282008936
  },
  "mmlu_high_school_statistics": {
    "alias": "  - high_school_statistics",
    "acc,none": 0.23148148148148148,
    "acc_stderr,none": 0.028765111718046934
  },
  "mmlu_machine_learning": {
    "alias": "  - machine_learning",
    "acc,none": 0.32142857142857145,
    "acc_stderr,none": 0.04432804055291519
  },
  "winogrande": {
    "alias": "winogrande",
    "acc,none": 0.47987371744277824,
    "acc_stderr,none": 0.014041096664344346
  }
}
{
  "arc_challenge": {
    "alias": "arc_challenge",
    "acc,none": 0.23122866894197952,
    "acc_stderr,none": 0.01232085883477234,
    "acc_norm,none": 0.24573378839590443,
    "acc_norm_stderr,none": 0.012581033453730187
  },
  "hellaswag": {
    "alias": "hellaswag",
    "acc,none": 0.31248755228042224,
    "acc_stderr,none": 0.004625600916775252,
    "acc_norm,none": 0.3738299143596893,
    "acc_norm_stderr,none": 0.004828305041904786
  },
  "mmlu": {
    "acc,none": 0.3235294117647059,
    "acc_stderr,none": 0.003924750679367415,
    "alias": "mmlu"
  },
  "mmlu_humanities": {
    "acc,none": 0.2964930924548353,
    "acc_stderr,none": 0.006641173140468088,
    "alias": " - humanities"
  },
  "mmlu_formal_logic": {
    "alias": "  - formal_logic",
    "acc,none": 0.3253968253968254,
    "acc_stderr,none": 0.04190596438871138
  },
  "mmlu_high_school_european_history": {
    "alias": "  - high_school_european_history",
    "acc,none": 0.38181818181818183,
    "acc_stderr,none": 0.03793713171165631
  },
  "mmlu_high_school_us_history": {
    "alias": "  - high_school_us_history",
    "acc,none": 0.30392156862745096,
    "acc_stderr,none": 0.03228210387037892
  },
  "mmlu_high_school_world_history": {
    "alias": "  - high_school_world_history",
    "acc,none": 0.3206751054852321,
    "acc_stderr,none": 0.03038193194999035
  },
  "mmlu_international_law": {
    "alias": "  - international_law",
    "acc,none": 0.2644628099173554,
    "acc_stderr,none": 0.04026187527591209
  },
  "mmlu_jurisprudence": {
    "alias": "  - jurisprudence",
    "acc,none": 0.2962962962962963,
    "acc_stderr,none": 0.04414343666854928
  },
  "mmlu_logical_fallacies": {
    "alias": "  - logical_fallacies",
    "acc,none": 0.3496932515337423,
    "acc_stderr,none": 0.0374666832547002
  },
  "mmlu_moral_disputes": {
    "alias": "  - moral_disputes",
    "acc,none": 0.3265895953757225,
    "acc_stderr,none": 0.02524826477424287
  },
  "mmlu_moral_scenarios": {
    "alias": "  - moral_scenarios",
    "acc,none": 0.2424581005586592,
    "acc_stderr,none": 0.01433352205921795
  },
  "mmlu_philosophy": {
    "alias": "  - philosophy",
    "acc,none": 0.36012861736334406,
    "acc_stderr,none": 0.027264297599803998
  },
  "mmlu_prehistory": {
    "alias": "  - prehistory",
    "acc,none": 0.35185185185185186,
    "acc_stderr,none": 0.026571483480719974
  },
  "mmlu_professional_law": {
    "alias": "  - professional_law",
    "acc,none": 0.2790091264667536,
    "acc_stderr,none": 0.011455208832803503
  },
  "mmlu_world_religions": {
    "alias": "  - world_religions",
    "acc,none": 0.2807017543859649,
    "acc_stderr,none": 0.03446296217088429
  },
  "mmlu_other": {
    "acc,none": 0.35661409719987125,
    "acc_stderr,none": 0.008527590473945244,
    "alias": " - other"
  },
  "mmlu_business_ethics": {
    "alias": "  - business_ethics",
    "acc,none": 0.35,
    "acc_stderr,none": 0.04793724854411023
  },
  "mmlu_clinical_knowledge": {
    "alias": "  - clinical_knowledge",
    "acc,none": 0.37735849056603776,
    "acc_stderr,none": 0.029832808114796033
  },
  "mmlu_college_medicine": {
    "alias": "  - college_medicine",
    "acc,none": 0.35260115606936415,
    "acc_stderr,none": 0.03643037168958545
  },
  "mmlu_global_facts": {
    "alias": "  - global_facts",
    "acc,none": 0.28,
    "acc_stderr,none": 0.045126085985421296
  },
  "mmlu_human_aging": {
    "alias": "  - human_aging",
    "acc,none": 0.27802690582959644,
    "acc_stderr,none": 0.030069584874494053
  },
  "mmlu_management": {
    "alias": "  - management",
    "acc,none": 0.4563106796116505,
    "acc_stderr,none": 0.04931801994220418
  },
  "mmlu_marketing": {
    "alias": "  - marketing",
    "acc,none": 0.49145299145299143,
    "acc_stderr,none": 0.03275130300097023
  },
  "mmlu_medical_genetics": {
    "alias": "  - medical_genetics",
    "acc,none": 0.35,
    "acc_stderr,none": 0.04793724854411023
  },
  "mmlu_miscellaneous": {
    "alias": "  - miscellaneous",
    "acc,none": 0.38058748403575987,
    "acc_stderr,none": 0.01736256412607552
  },
  "mmlu_nutrition": {
    "alias": "  - nutrition",
    "acc,none": 0.38562091503267976,
    "acc_stderr,none": 0.02787074527829027
  },
  "mmlu_professional_accounting": {
    "alias": "  - professional_accounting",
    "acc,none": 0.29432624113475175,
    "acc_stderr,none": 0.027187127011503866
  },
  "mmlu_professional_medicine": {
    "alias": "  - professional_medicine",
    "acc,none": 0.22794117647058823,
    "acc_stderr,none": 0.02548308146802982
  },
  "mmlu_virology": {
    "alias": "  - virology",
    "acc,none": 0.3855421686746988,
    "acc_stderr,none": 0.03789134424611549
  },
  "mmlu_social_sciences": {
    "acc,none": 0.3461163470913227,
    "acc_stderr,none": 0.008528680648625657,
    "alias": " - social sciences"
  },
  "mmlu_econometrics": {
    "alias": "  - econometrics",
    "acc,none": 0.21929824561403508,
    "acc_stderr,none": 0.038924311065187546
  },
  "mmlu_high_school_geography": {
    "alias": "  - high_school_geography",
    "acc,none": 0.40404040404040403,
    "acc_stderr,none": 0.03496130972056125
  },
  "mmlu_high_school_government_and_politics": {
    "alias": "  - high_school_government_and_politics",
    "acc,none": 0.30569948186528495,
    "acc_stderr,none": 0.03324837939758158
  },
  "mmlu_high_school_macroeconomics": {
    "alias": "  - high_school_macroeconomics",
    "acc,none": 0.32564102564102565,
    "acc_stderr,none": 0.023759665767412275
  },
  "mmlu_high_school_microeconomics": {
    "alias": "  - high_school_microeconomics",
    "acc,none": 0.35294117647058826,
    "acc_stderr,none": 0.03104194130405932
  },
  "mmlu_high_school_psychology": {
    "alias": "  - high_school_psychology",
    "acc,none": 0.43486238532110094,
    "acc_stderr,none": 0.021254631465609235
  },
  "mmlu_human_sexuality": {
    "alias": "  - human_sexuality",
    "acc,none": 0.3511450381679389,
    "acc_stderr,none": 0.041864451630137474
  },
  "mmlu_professional_psychology": {
    "alias": "  - professional_psychology",
    "acc,none": 0.2973856209150327,
    "acc_stderr,none": 0.01849259653639692
  },
  "mmlu_public_relations": {
    "alias": "  - public_relations",
    "acc,none": 0.3181818181818182,
    "acc_stderr,none": 0.044612721759105065
  },
  "mmlu_security_studies": {
    "alias": "  - security_studies",
    "acc,none": 0.2938775510204082,
    "acc_stderr,none": 0.02916273841024971
  },
  "mmlu_sociology": {
    "alias": "  - sociology",
    "acc,none": 0.4129353233830846,
    "acc_stderr,none": 0.03481520803367344
  },
  "mmlu_us_foreign_policy": {
    "alias": "  - us_foreign_policy",
    "acc,none": 0.35,
    "acc_stderr,none": 0.04793724854411023
  },
  "mmlu_stem": {
    "acc,none": 0.30922930542340626,
    "acc_stderr,none": 0.008211001111562047,
    "alias": " - stem"
  },
  "mmlu_abstract_algebra": {
    "alias": "  - abstract_algebra",
    "acc,none": 0.31,
    "acc_stderr,none": 0.04648231987117317
  },
  "mmlu_anatomy": {
    "alias": "  - anatomy",
    "acc,none": 0.34074074074074073,
    "acc_stderr,none": 0.04094376269996792
  },
  "mmlu_astronomy": {
    "alias": "  - astronomy",
    "acc,none": 0.2631578947368421,
    "acc_stderr,none": 0.03583496176361067
  },
  "mmlu_college_biology": {
    "alias": "  - college_biology",
    "acc,none": 0.3055555555555556,
    "acc_stderr,none": 0.038520846960085356
  },
  "mmlu_college_chemistry": {
    "alias": "  - college_chemistry",
    "acc,none": 0.25,
    "acc_stderr,none": 0.04351941398892446
  },
  "mmlu_college_computer_science": {
    "alias": "  - college_computer_science",
    "acc,none": 0.27,
    "acc_stderr,none": 0.04461960433384737
  },
  "mmlu_college_mathematics": {
    "alias": "  - college_mathematics",
    "acc,none": 0.32,
    "acc_stderr,none": 0.04688261722621507
  },
  "mmlu_college_physics": {
    "alias": "  - college_physics",
    "acc,none": 0.22549019607843138,
    "acc_stderr,none": 0.04158307533083289
  },
  "mmlu_computer_security": {
    "alias": "  - computer_security",
    "acc,none": 0.38,
    "acc_stderr,none": 0.04878317312145634
  },
  "mmlu_conceptual_physics": {
    "alias": "  - conceptual_physics",
    "acc,none": 0.34893617021276596,
    "acc_stderr,none": 0.031158522131357808
  },
  "mmlu_electrical_engineering": {
    "alias": "  - electrical_engineering",
    "acc,none": 0.32413793103448274,
    "acc_stderr,none": 0.03900432069185552
  },
  "mmlu_elementary_mathematics": {
    "alias": "  - elementary_mathematics",
    "acc,none": 0.2777777777777778,
    "acc_stderr,none": 0.0230681888482611
  },
  "mmlu_high_school_biology": {
    "alias": "  - high_school_biology",
    "acc,none": 0.4064516129032258,
    "acc_stderr,none": 0.027941727346256367
  },
  "mmlu_high_school_chemistry": {
    "alias": "  - high_school_chemistry",
    "acc,none": 0.31527093596059114,
    "acc_stderr,none": 0.03269080871970191
  },
  "mmlu_high_school_computer_science": {
    "alias": "  - high_school_computer_science",
    "acc,none": 0.34,
    "acc_stderr,none": 0.04760952285695233
  },
  "mmlu_high_school_mathematics": {
    "alias": "  - high_school_mathematics",
    "acc,none": 0.3074074074074074,
    "acc_stderr,none": 0.028133252578815583
  },
  "mmlu_high_school_physics": {
    "alias": "  - high_school_physics",
    "acc,none": 0.2251655629139073,
    "acc_stderr,none": 0.03410435282008936
  },
  "mmlu_high_school_statistics": {
    "alias": "  - high_school_statistics",
    "acc,none": 0.2777777777777778,
    "acc_stderr,none": 0.030546745264953202
  },
  "mmlu_machine_learning": {
    "alias": "  - machine_learning",
    "acc,none": 0.30357142857142855,
    "acc_stderr,none": 0.043642261558410424
  },
  "winogrande": {
    "alias": "winogrande",
    "acc,none": 0.5240726124704025,
    "acc_stderr,none": 0.014036189665395169
  }
}
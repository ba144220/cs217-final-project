{
  "arc_challenge": {
    "alias": "arc_challenge",
    "acc,none": 0.29436860068259385,
    "acc_stderr,none": 0.013318528460539387,
    "acc_norm,none": 0.30204778156996587,
    "acc_norm_stderr,none": 0.013417519144716299
  },
  "hellaswag": {
    "alias": "hellaswag",
    "acc,none": 0.3593905596494722,
    "acc_stderr,none": 0.004788412062375877,
    "acc_norm,none": 0.45130452101175067,
    "acc_norm_stderr,none": 0.004966060995315382
  },
  "mmlu": {
    "acc,none": 0.3799316336704173,
    "acc_stderr,none": 0.004049642252406568,
    "alias": "mmlu"
  },
  "mmlu_humanities": {
    "acc,none": 0.3341126461211477,
    "acc_stderr,none": 0.006832462357345352,
    "alias": " - humanities"
  },
  "mmlu_formal_logic": {
    "alias": "  - formal_logic",
    "acc,none": 0.42063492063492064,
    "acc_stderr,none": 0.044154382267437474
  },
  "mmlu_high_school_european_history": {
    "alias": "  - high_school_european_history",
    "acc,none": 0.4666666666666667,
    "acc_stderr,none": 0.0389565806527185
  },
  "mmlu_high_school_us_history": {
    "alias": "  - high_school_us_history",
    "acc,none": 0.39705882352941174,
    "acc_stderr,none": 0.034341311647191244
  },
  "mmlu_high_school_world_history": {
    "alias": "  - high_school_world_history",
    "acc,none": 0.43037974683544306,
    "acc_stderr,none": 0.03223017195937597
  },
  "mmlu_international_law": {
    "alias": "  - international_law",
    "acc,none": 0.4049586776859504,
    "acc_stderr,none": 0.04481137755942466
  },
  "mmlu_jurisprudence": {
    "alias": "  - jurisprudence",
    "acc,none": 0.37037037037037035,
    "acc_stderr,none": 0.04668408033024929
  },
  "mmlu_logical_fallacies": {
    "alias": "  - logical_fallacies",
    "acc,none": 0.37423312883435583,
    "acc_stderr,none": 0.03802068102899615
  },
  "mmlu_moral_disputes": {
    "alias": "  - moral_disputes",
    "acc,none": 0.3236994219653179,
    "acc_stderr,none": 0.02519018132760835
  },
  "mmlu_moral_scenarios": {
    "alias": "  - moral_scenarios",
    "acc,none": 0.27150837988826815,
    "acc_stderr,none": 0.01487425216809521
  },
  "mmlu_philosophy": {
    "alias": "  - philosophy",
    "acc,none": 0.41479099678456594,
    "acc_stderr,none": 0.027982680459759515
  },
  "mmlu_prehistory": {
    "alias": "  - prehistory",
    "acc,none": 0.3611111111111111,
    "acc_stderr,none": 0.026725868809100835
  },
  "mmlu_professional_law": {
    "alias": "  - professional_law",
    "acc,none": 0.29335071707953064,
    "acc_stderr,none": 0.011628520449582173
  },
  "mmlu_world_religions": {
    "alias": "  - world_religions",
    "acc,none": 0.3391812865497076,
    "acc_stderr,none": 0.03631053496488902
  },
  "mmlu_other": {
    "acc,none": 0.3855809462504023,
    "acc_stderr,none": 0.008594020013429262,
    "alias": " - other"
  },
  "mmlu_business_ethics": {
    "alias": "  - business_ethics",
    "acc,none": 0.34,
    "acc_stderr,none": 0.04760952285695233
  },
  "mmlu_clinical_knowledge": {
    "alias": "  - clinical_knowledge",
    "acc,none": 0.41132075471698115,
    "acc_stderr,none": 0.030285009259009833
  },
  "mmlu_college_medicine": {
    "alias": "  - college_medicine",
    "acc,none": 0.4046242774566474,
    "acc_stderr,none": 0.037424611938872455
  },
  "mmlu_global_facts": {
    "alias": "  - global_facts",
    "acc,none": 0.18,
    "acc_stderr,none": 0.03861229196653691
  },
  "mmlu_human_aging": {
    "alias": "  - human_aging",
    "acc,none": 0.21076233183856502,
    "acc_stderr,none": 0.02737309550054016
  },
  "mmlu_management": {
    "alias": "  - management",
    "acc,none": 0.5242718446601942,
    "acc_stderr,none": 0.049449010929737795
  },
  "mmlu_marketing": {
    "alias": "  - marketing",
    "acc,none": 0.5683760683760684,
    "acc_stderr,none": 0.03244835535311493
  },
  "mmlu_medical_genetics": {
    "alias": "  - medical_genetics",
    "acc,none": 0.36,
    "acc_stderr,none": 0.048241815132442176
  },
  "mmlu_miscellaneous": {
    "alias": "  - miscellaneous",
    "acc,none": 0.39719029374201786,
    "acc_stderr,none": 0.01749790503715936
  },
  "mmlu_nutrition": {
    "alias": "  - nutrition",
    "acc,none": 0.4117647058823529,
    "acc_stderr,none": 0.028180596328259325
  },
  "mmlu_professional_accounting": {
    "alias": "  - professional_accounting",
    "acc,none": 0.3120567375886525,
    "acc_stderr,none": 0.027640120545169986
  },
  "mmlu_professional_medicine": {
    "alias": "  - professional_medicine",
    "acc,none": 0.45588235294117646,
    "acc_stderr,none": 0.030254372573976694
  },
  "mmlu_virology": {
    "alias": "  - virology",
    "acc,none": 0.2891566265060241,
    "acc_stderr,none": 0.035294868015111204
  },
  "mmlu_social_sciences": {
    "acc,none": 0.4289892752681183,
    "acc_stderr,none": 0.00884359375881274,
    "alias": " - social sciences"
  },
  "mmlu_econometrics": {
    "alias": "  - econometrics",
    "acc,none": 0.2894736842105263,
    "acc_stderr,none": 0.04266339443159392
  },
  "mmlu_high_school_geography": {
    "alias": "  - high_school_geography",
    "acc,none": 0.494949494949495,
    "acc_stderr,none": 0.03562170760625402
  },
  "mmlu_high_school_government_and_politics": {
    "alias": "  - high_school_government_and_politics",
    "acc,none": 0.43005181347150256,
    "acc_stderr,none": 0.03572954333144807
  },
  "mmlu_high_school_macroeconomics": {
    "alias": "  - high_school_macroeconomics",
    "acc,none": 0.41794871794871796,
    "acc_stderr,none": 0.02500732988246114
  },
  "mmlu_high_school_microeconomics": {
    "alias": "  - high_school_microeconomics",
    "acc,none": 0.47058823529411764,
    "acc_stderr,none": 0.032422250271150026
  },
  "mmlu_high_school_psychology": {
    "alias": "  - high_school_psychology",
    "acc,none": 0.5137614678899083,
    "acc_stderr,none": 0.021429202089874165
  },
  "mmlu_human_sexuality": {
    "alias": "  - human_sexuality",
    "acc,none": 0.40458015267175573,
    "acc_stderr,none": 0.04304693795380667
  },
  "mmlu_professional_psychology": {
    "alias": "  - professional_psychology",
    "acc,none": 0.32679738562091504,
    "acc_stderr,none": 0.018975427920507264
  },
  "mmlu_public_relations": {
    "alias": "  - public_relations",
    "acc,none": 0.36363636363636365,
    "acc_stderr,none": 0.04607582090719978
  },
  "mmlu_security_studies": {
    "alias": "  - security_studies",
    "acc,none": 0.4775510204081633,
    "acc_stderr,none": 0.03197694118713664
  },
  "mmlu_sociology": {
    "alias": "  - sociology",
    "acc,none": 0.5024875621890548,
    "acc_stderr,none": 0.03535490150137287
  },
  "mmlu_us_foreign_policy": {
    "alias": "  - us_foreign_policy",
    "acc,none": 0.4,
    "acc_stderr,none": 0.0492365963917331
  },
  "mmlu_stem": {
    "acc,none": 0.3948620361560419,
    "acc_stderr,none": 0.008667593083603353,
    "alias": " - stem"
  },
  "mmlu_abstract_algebra": {
    "alias": "  - abstract_algebra",
    "acc,none": 0.34,
    "acc_stderr,none": 0.04760952285695233
  },
  "mmlu_anatomy": {
    "alias": "  - anatomy",
    "acc,none": 0.37777777777777777,
    "acc_stderr,none": 0.041883075375958506
  },
  "mmlu_astronomy": {
    "alias": "  - astronomy",
    "acc,none": 0.506578947368421,
    "acc_stderr,none": 0.040685900502249725
  },
  "mmlu_college_biology": {
    "alias": "  - college_biology",
    "acc,none": 0.3333333333333333,
    "acc_stderr,none": 0.03942082639927217
  },
  "mmlu_college_chemistry": {
    "alias": "  - college_chemistry",
    "acc,none": 0.4,
    "acc_stderr,none": 0.0492365963917331
  },
  "mmlu_college_computer_science": {
    "alias": "  - college_computer_science",
    "acc,none": 0.45,
    "acc_stderr,none": 0.05
  },
  "mmlu_college_mathematics": {
    "alias": "  - college_mathematics",
    "acc,none": 0.35,
    "acc_stderr,none": 0.04793724854411023
  },
  "mmlu_college_physics": {
    "alias": "  - college_physics",
    "acc,none": 0.38235294117647056,
    "acc_stderr,none": 0.048355036961072254
  },
  "mmlu_computer_security": {
    "alias": "  - computer_security",
    "acc,none": 0.5,
    "acc_stderr,none": 0.050251890762960605
  },
  "mmlu_conceptual_physics": {
    "alias": "  - conceptual_physics",
    "acc,none": 0.40425531914893614,
    "acc_stderr,none": 0.032081157507886864
  },
  "mmlu_electrical_engineering": {
    "alias": "  - electrical_engineering",
    "acc,none": 0.4206896551724138,
    "acc_stderr,none": 0.0411391498118926
  },
  "mmlu_elementary_mathematics": {
    "alias": "  - elementary_mathematics",
    "acc,none": 0.3544973544973545,
    "acc_stderr,none": 0.024636830602842035
  },
  "mmlu_high_school_biology": {
    "alias": "  - high_school_biology",
    "acc,none": 0.4612903225806452,
    "acc_stderr,none": 0.02835863485983693
  },
  "mmlu_high_school_chemistry": {
    "alias": "  - high_school_chemistry",
    "acc,none": 0.3842364532019704,
    "acc_stderr,none": 0.03422398565657553
  },
  "mmlu_high_school_computer_science": {
    "alias": "  - high_school_computer_science",
    "acc,none": 0.44,
    "acc_stderr,none": 0.049888765156985884
  },
  "mmlu_high_school_mathematics": {
    "alias": "  - high_school_mathematics",
    "acc,none": 0.32222222222222224,
    "acc_stderr,none": 0.028493465091028538
  },
  "mmlu_high_school_physics": {
    "alias": "  - high_school_physics",
    "acc,none": 0.37748344370860926,
    "acc_stderr,none": 0.03958027231121572
  },
  "mmlu_high_school_statistics": {
    "alias": "  - high_school_statistics",
    "acc,none": 0.4537037037037037,
    "acc_stderr,none": 0.033953227263757976
  },
  "mmlu_machine_learning": {
    "alias": "  - machine_learning",
    "acc,none": 0.25892857142857145,
    "acc_stderr,none": 0.04157751539865629
  },
  "winogrande": {
    "alias": "winogrande",
    "acc,none": 0.5327545382794001,
    "acc_stderr,none": 0.014022300570434274
  }
}